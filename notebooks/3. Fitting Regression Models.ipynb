{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Regression modeling\n",
    "\n",
    "A general, primary goal of many statistical data analysis tasks is to relate the influence of one variable on another. \n",
    "\n",
    "For example: \n",
    "\n",
    "- how different medical interventions influence the incidence or duration of disease\n",
    "- how baseball player's performance varies as a function of age.\n",
    "- [how test scores are correlated with tissue LSD concentration](http://onlinelibrary.wiley.com/doi/10.1002/cpt196895635/abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "data_string = \"\"\"\n",
    "Drugs\tScore\n",
    "0\t1.17\t78.93\n",
    "1\t2.97\t58.20\n",
    "2\t3.26\t67.47\n",
    "3\t4.69\t37.47\n",
    "4\t5.83\t45.65\n",
    "5\t6.00\t32.92\n",
    "6\t6.41\t29.97\n",
    "\"\"\"\n",
    "\n",
    "lsd_and_math = pd.read_table(StringIO(data_string), sep='\\t', index_col=0)\n",
    "lsd_and_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lsd](images/_lsd.jpg)\n",
    "\n",
    "> Taking LSD was a profound experience, one of the most important things in my life --Steve Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsd_and_math.plot(x='Drugs', y='Score', style='ro', legend=False, xlim=(0,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a model to characterize the relationship between $X$ and $Y$, recognizing that additional factors other than $X$ (the ones we have measured or are interested in) may influence the response variable $Y$.\n",
    "\n",
    "- $M(Y|X) = E(Y|X)$\n",
    "- $M(Y|X) = Pr(Y=1|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general,\n",
    "\n",
    "$$M(Y|X) = f(X)$$\n",
    "\n",
    "for a regression model,\n",
    "\n",
    "$$M(Y|X) = f(X\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f$ is some function, for example a linear function:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_{1i} + \\ldots + \\beta_k x_{ki} + \\epsilon_i$\n",
    "</div>\n",
    "\n",
    "Regression is a **weighted sum** of independent predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\epsilon_i$ accounts for the difference between the observed response $y_i$ and its prediction from the model $\\hat{y_i} = \\beta_0 + \\beta_1 x_i$. This is sometimes referred to as **process uncertainty**.\n",
    "\n",
    "Interpretation: coefficients represent the change in Y for a unit increment of the predictor X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important linear regression **assumptions**:\n",
    "\n",
    "1. normal errors\n",
    "2. homoscedasticity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter estimation\n",
    "\n",
    "We would like to select $\\beta_0, \\beta_1$ so that the difference between the predictions and the observations is zero, but this is not usually possible. Instead, we choose a reasonable criterion: ***the smallest sum of the squared differences between $\\hat{y}$ and $y$***.\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$R^2 = \\sum_i (y_i - [\\beta_0 + \\beta_1 x_i])^2 = \\sum_i \\epsilon_i^2 $$  \n",
    "</div>\n",
    "\n",
    "Squaring serves two purposes: \n",
    "\n",
    "1. to prevent positive and negative values from cancelling each other out\n",
    "2. to strongly penalize large deviations. \n",
    "\n",
    "Whether or not the latter is a desired depends on the goals of the analysis.\n",
    "\n",
    "In other words, we will select the parameters that minimize the squared error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_of_squares = lambda θ, x, y: np.sum((y - θ[0] - θ[1]*x) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample calculation, using aribitrary parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_of_squares([0,1], lsd_and_math.Drugs, lsd_and_math.Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have the stated objective of minimizing the sum of squares, so we can pass this function to one of several optimizers in SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = lsd_and_math.T.values\n",
    "b0, b1 = fmin(sum_of_squares, [0,1], args=(x,y))\n",
    "b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = lsd_and_math.plot(x='Drugs', y='Score', style='ro', legend=False, xlim=(0,8), ylim=(20, 90))\n",
    "ax.plot([0,10], [b0, b0+b1*10])\n",
    "for xi, yi in zip(x,y):\n",
    "    ax.plot([xi]*2, [yi, b0+b1*xi], 'k:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative loss functions\n",
    "\n",
    "Minimizing the sum of squares is not the only criterion we can use; it is just a very popular (and successful) one. For example, we can try to minimize the sum of absolute differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_of_absval = lambda θ, x, y: np.sum(np.abs(y - θ[0] - θ[1]*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b0, b1 = fmin(sum_of_absval, [0,0], args=(x,y))\n",
    "print('\\nintercept: {0:.2}, slope: {1:.2}'.format(b0,b1))\n",
    "ax = lsd_and_math.plot(x='Drugs', y='Score', style='ro', legend=False, xlim=(0,8))\n",
    "ax.plot([0,10], [b0, b0+b1*10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Append an additional, extreme observation: `{'Drugs': 6, 'Score': 70}`.\n",
    "\n",
    "2. Re-fit the model using both the sum of squares error and sum of absolute values error, and compare the resulting estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regession\n",
    "\n",
    "We are not restricted to a straight-line regression model; we can represent a curved relationship between our variables by introducing **polynomial** terms. For example, a cubic model:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_squares_quad = lambda θ, x, y: np.sum((y - θ[0] - θ[1]*x - θ[2]*(x**2)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b0,b1,b2 = fmin(sum_squares_quad, [1,1,-1], args=(x,y))\n",
    "print('\\nintercept: {0:.2}, x: {1:.2}, x2: {2:.2}'.format(b0,b1,b2))\n",
    "ax = lsd_and_math.plot(x='Drugs', y='Score', style='ro', legend=False, xlim=(0,8))\n",
    "xvals = np.linspace(0, 8, 100)\n",
    "ax.plot(xvals, b0 + b1*xvals + b2*(xvals**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a polynomial model characterizes a nonlinear relationship, it is a linear problem in terms of estimation. That is, the regression model $f(y | x)$ is **linear in the parameters**.\n",
    "\n",
    "For some data, it may be reasonable to consider polynomials of order>2. For example, consider the relationship between the number of spawning salmon and the number of juveniles recruited into the population the following year; one would expect the relationship to be positive, but not necessarily linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salmon = pd.read_table(\"../data/salmon.dat\", delim_whitespace=True, index_col=0)\n",
    "salmon.plot.scatter(x='spawners', y='recruits');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a sum of squares function for a cubic regression (order=3), and fit this function to the salmon spawning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression with PyMC3\n",
    "\n",
    "In practice, we need not fit least squares models by hand because they are implemented generally in packages such as [`scikit-learn`](http://scikit-learn.org/) and [`statsmodels`](https://github.com/statsmodels/statsmodels/). Moreover, we are interested not only in obtaining a line of best fit, but also **estimates of uncertainty** in the line and the parameters used to calculate the line.\n",
    "\n",
    "Here, we will return to a Bayesian approach and build a regression model in PyMC3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors\n",
    "\n",
    "The first step in specifying our model is to specify priors for our model. Since regression parameters are continuous, and potentially positive or negative, we can use a Normal distribution with a variance set to an appropriate value that reflects our prior knowledsge of the parameter.\n",
    "\n",
    "$$\\beta_i \\sim \\text{Normal}(0, 100)$$\n",
    "\n",
    "The other latent variable is the residual variance of the observations after applying our model. This is the **process uncertainty** that we identified earlier. \n",
    "\n",
    "$$\\sigma \\sim \\text{HalfCauchy}(1)$$\n",
    "\n",
    "The **half-Cauchy** distribution used here provides support over positive continuous values, and has relatively large tail probabilities, allowing for the possibility of extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import HalfCauchy\n",
    "\n",
    "ax = sns.kdeplot(HalfCauchy.dist(1).random(size=10000), gridsize=2000)\n",
    "ax.set_xlim(0, 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Normal, Model\n",
    "\n",
    "with Model() as drugs_model:\n",
    "    \n",
    "    intercept = Normal('intercept', 0, sd=100)\n",
    "    slope = Normal('slope', 0, sd=100)\n",
    "    σ = HalfCauchy('σ', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Likelihood\n",
    "\n",
    "The sampling distribution of the data for a regression model is a normal distribution, and we specified the standard deviation for this sampling distribution in $\\sigma$ above. \n",
    "\n",
    "$$y_i \\sim \\text{Normal}(\\mu_i, \\sigma)$$\n",
    "\n",
    "Here, $\\mu_i$ is the expected value of the *i*th observation, which is generated by the regression model at the corresponding value of $x$. We can calculate this expected value as a function of the regression parameters and the data, and pass it to the normal likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with drugs_model:\n",
    "    \n",
    "    μ = intercept + slope*x\n",
    "    score = Normal('score', μ, sd=σ, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!\n",
    "\n",
    "The regression model is fully specified with these 5 lines of Python code. We can now use the fitting method of our choice to estimate a posterior distribution. In the previous module, we used variational inference, here we will use a **Markov chain Monte Carlo** algorithm, called **NUTS** (the No U-Turn Sampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import sample\n",
    "\n",
    "with drugs_model:\n",
    "\n",
    "    drugs_sample = sample(1000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import plot_posterior\n",
    "\n",
    "plot_posterior(drugs_sample, varnames=['slope']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have a vector of samples from the estimated posterior, it is easy to calculate means, medians, standard deviations, and probability intervals. Above, we see the mean and the 95% **posterior credible interval** reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Fit a quadratic model for the salmon spawning data, using PyMC3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model fit\n",
    "\n",
    "One intuitive way of evaluating model fit is to compare model predictions with the observations used to fit the model. In other words, the fitted model can be used to **simulate\n",
    "data**, and the distribution of the simulated data should resemble the distribution of the actual data.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of the Bayesian modelling framework. Recall, from our introduction to Bayesian inference, the **posterior predictive distribution**:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is straighforward in PyMC; the `sample_ppc` function draws posterior predictive checks from all of the data likelihoods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import sample_ppc\n",
    "\n",
    "with drugs_model:\n",
    "    \n",
    "    drugs_ppc = sample_ppc(drugs_sample, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields 1000 samples corresponding to each of the seven data points in our observation vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drugs_ppc['score'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare these simulated data to the data we used to fit the model. Our claim is that the model might have plausibly been used to generate the data that we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for ax, real_data, sim_data in zip(axes_flat[:-1], y, drugs_ppc['score'].T):\n",
    "    ax.hist(sim_data, bins=20)\n",
    "    ax.vlines(real_data, *ax.get_ylim(), colors='red')\n",
    "    ax.set_yticklabels([])\n",
    "    sns.despine(left=True)\n",
    "\n",
    "axes_flat[-1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized linear models\n",
    "\n",
    "Often our data violates one or more of the linear regression assumptions:\n",
    "\n",
    "- non-linear\n",
    "- non-normal error distribution\n",
    "- heteroskedasticity\n",
    "\n",
    "this forces us to **generalize** the regression model in order to account for these characteristics.\n",
    "\n",
    "As a motivating example, consider the Olympic medals data that we compiled earlier in the tutorial.\n",
    "\n",
    "![relay](images/mascot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medals = pd.read_csv('../data/medals.csv')\n",
    "medals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a positive relationship between population and awarded medals, but the data in their raw form are clearly not amenable to linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medals.plot(x='population', y='medals', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the issue is the scale of the variables. For example, countries' populations span several orders of magnitude. We can correct this by using the logarithm of population, which we have already calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medals.plot(x='log_population', y='medals', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an improvement, but the relationship is still not adequately modeled by least-squares regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import fit\n",
    "\n",
    "x = medals.log_population.values\n",
    "y = medals.medals.values\n",
    "\n",
    "with Model() as medals_linear:\n",
    "    \n",
    "    intercept = Normal('intercept', 0, sd=100)\n",
    "    slope = Normal('slope', 0, sd=100)\n",
    "    σ = HalfCauchy('σ', 1)\n",
    "    \n",
    "    μ = intercept + slope*x\n",
    "    score = Normal('score', μ, sd=σ, observed=y)\n",
    "    \n",
    "    samples = sample(500, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat = samples['intercept'].mean() + samples['slope'].mean() * np.linspace(10, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = medals.plot(x='log_population', y='medals', kind='scatter')\n",
    "ax.plot(np.linspace(10, 22), yhat, color='red',\n",
    "         linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is due to the fact that the response data are **counts**. As a result, they tend to have characteristic properties. \n",
    "\n",
    "- discrete\n",
    "- positive\n",
    "- variance grows with mean\n",
    "\n",
    "to account for this, we can do two things: (1) model the medal count on the **log scale** and (2) assume **Poisson errors**, rather than normal.\n",
    "\n",
    "Recall the Poisson distribution from the previous section:\n",
    "\n",
    "$$p(y)=\\frac{e^{-\\lambda}\\lambda^y}{y!}$$\n",
    "\n",
    "* $Y=\\{0,1,2,\\ldots\\}$\n",
    "* $\\lambda > 0$\n",
    "\n",
    "$$E(Y) = \\text{Var}(Y) = \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will model the logarithm of the expected value as a linear function of our predictors:\n",
    "\n",
    "$$\\log(\\lambda) = X\\beta$$\n",
    "\n",
    "In this context, the log function is called a **link function**. This transformation implies the mean of the Poisson is:\n",
    "\n",
    "$$\\lambda = \\exp(X\\beta)$$\n",
    "\n",
    "We can plug this into the Poisson likelihood and use maximum likelihood to estimate the regression covariates $\\beta$.\n",
    "\n",
    "$$\\log L = \\sum_{i=1}^n -\\exp(X_i\\beta) + Y_i (X_i \\beta)- \\log(Y_i!)$$\n",
    "\n",
    "As we have already done, we just need to code the kernel of this likelihood, and optimize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Poisson negative log-likelhood\n",
    "poisson_loglike = lambda β, X, y: -(-np.exp(X.dot(β)) + y*X.dot(β)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `assign` method to add a column of ones to the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poisson_loglike([0,1], medals[['log_population']].assign(intercept=1), medals.medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Nelder-Mead to minimize the negtive log-likelhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1, b0 = fmin(poisson_loglike, [0,1], args=(medals[['log_population']].assign(intercept=1).values, \n",
    "                                            medals.medals.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b0, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting fit looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = medals.plot(x='log_population', y='medals', kind='scatter')\n",
    "xvals = np.arange(12, 22)\n",
    "ax.plot(xvals, np.exp(b0 + b1*xvals), 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Poisson regression\n",
    "\n",
    "Code the Poisson regression model above in PyMC3 (*Hint*: you will need to specify a Poisson distribution somewhere!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Multivariate GLM\n",
    "\n",
    "Add the OECD indicator variable to the model, and estimate the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Fitting a line to the relationship between two variables using the least squares approach is sensible when the variable we are trying to predict is continuous, but what about when the data are dichotomous?\n",
    "\n",
    "- male/female\n",
    "- pass/fail\n",
    "- died/survived\n",
    "\n",
    "Let's revisit the *very low birthweight infants* dataset from the last module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vlbw = pd.read_csv('../data/vlbw.csv', index_col=0).dropna(axis=0, subset=['ivh', 'pneumo'])\n",
    "ivh = vlbw.ivh.isin(['definite', 'possible']).astype(int).values\n",
    "pneumo = vlbw.pneumo.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we were comparing the number of **intra-ventricular hemorrhage** events between two groups, those with and without a pneumothorax, where we found a higher probability of IVH associated with pneumothorax. However, its possible that  lower birthweight accounts for the difference, since birthweight is nominally lower in the pneumothorax group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vlbw.groupby('pneumo').bwt.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the relative number of events as a function of birthweight, then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bwt_kg = vlbw.bwt.values/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jitter = np.random.normal(scale=0.02, size=len(vlbw))\n",
    "\n",
    "plt.scatter(bwt_kg, ivh + jitter, alpha=0.3)\n",
    "plt.yticks([0,1])\n",
    "plt.ylabel(\"IVH\")\n",
    "plt.xlabel(\"Birth weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have added random jitter on the y-axis to help visualize the density of the points, and have plotted fare x-axis.\n",
    "\n",
    "Clearly, fitting a line through this data makes little sense, for several reasons. First, for most values of the predictor variable, the line would predict values that are not zero or one. Second, it would seem odd to choose least squares (or similar) as a criterion for selecting the best line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betas_vlbw = fmin(sum_of_squares, [1,1], args=(bwt_kg,ivh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(bwt_kg, ivh + jitter, alpha=0.3)\n",
    "plt.yticks([0,1])\n",
    "plt.ylabel(\"IVH\")\n",
    "plt.xlabel(\"Birth weight\")\n",
    "plt.plot([0.4,1.6], [betas_vlbw[0] + betas_vlbw[1]*0.4, betas_vlbw[0] + betas_vlbw[1]*1.6], 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic model\n",
    "\n",
    "Rather than model the binary outcome explicitly, it makes sense instead to model the *probability* of death or survival in a **stochastic** model. Probabilities are measured on a continuous [0,1] scale, which may be more amenable for prediction using a regression line. We need to consider a different probability model for this exerciese however; let's consider the **Bernoulli** distribution as a generative model for our data:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$f(y|p) = p^{y} (1-p)^{1-y}$$ \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y = \\{0,1\\}$ and $p \\in [0,1]$. So, this model predicts whether $y$ is zero or one as a function of the probability $p$. Notice that when $y=1$, the $1-p$ term disappears, and when $y=0$, the $p$ term disappears.\n",
    "\n",
    "So, the model we want to fit should look something like this:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$p_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since $p$ is constrained to be between zero and one, it is easy to see where a linear (or polynomial) model might predict values outside of this range. As with the Poisson regression, we can modify this model slightly by using a link function to transform the probability to have an unbounded range on a new scale. Specifically, we can use a **logit transformation** as our link function:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$\\text{logit}(p) = \\log\\left[\\frac{p}{1-p}\\right] = x$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of $p/(1-p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = lambda p: np.log(p/(1.-p))\n",
    "unit_interval = np.linspace(0,1)\n",
    "plt.plot(unit_interval/(1-unit_interval), unit_interval)\n",
    "plt.xlabel(r'$p/(1-p)$')\n",
    "plt.ylabel('p');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the logit function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(logit(unit_interval), unit_interval)\n",
    "plt.xlabel('logit(p)')\n",
    "plt.ylabel('p');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of the logit transformation is:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$p = \\frac{1}{1 + \\exp(-x)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invlogit = lambda x: 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now our model is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$\\text{logit}(p_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this model using maximum likelihood. Our likelihood, again based on the Bernoulli model is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$L(y|p) = \\prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, on the log scale is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$l(y|p) = \\sum_{i=1}^n y_i \\log(p_i) + (1-y_i)\\log(1-p_i)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily implement this in Python, keeping in mind that `fmin` minimizes, rather than maximizes functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_like(θ, x, y):\n",
    "    \n",
    "    p = invlogit(θ[0] + θ[1] * x)\n",
    "    \n",
    "    # Return negative of log-likelihood\n",
    "    return -np.sum(y * np.log(p) + (1-y) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b0, b1 = fmin(logistic_like, [0.5,0], args=(bwt_kg, ivh))\n",
    "b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jitter = np.random.normal(scale=0.01, size=len(x))\n",
    "plt.plot(x, ivh+jitter, 'r.', alpha=0.3)\n",
    "plt.yticks([0,.25,.5,.75,1])\n",
    "xvals = np.linspace(0.4, 1.6)\n",
    "plt.plot(xvals, invlogit(b0+b1*xvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is just another type of GLM, this time with a Bernoulli distribution representing the sampling distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian multivariate logistic regression\n",
    "\n",
    "Let's once again drop our hand-fit model into a Bayesian context. This time we will model two variables, one discrete (pneumothorax) and one continuous (birth weight). First, we will center birth weight to aid interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bwt_centered = bwt_kg - bwt_kg.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our regression model for predicting the latent probability of IVH for each individual will be: \n",
    "\n",
    "$$p_i = \\mu + \\alpha * \\text{bwt}_i + \\beta * \\text{pneumo}_i$$\n",
    "\n",
    "here, $\\mu$ is a baseline probability, $\\alpha$ is a coefficient for centered birthweight, and $\\beta$ is a coefficient for pneumothorax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then step through the procedure for coding a Bayesian model, first by specifing priors for the linear model coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with Model() as ivh_glm:\n",
    "    \n",
    "    μ = Normal('μ', 0, sd=10)\n",
    "    α = Normal('α', 0, sd=10)\n",
    "    β = Normal('β', 0, sd=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, apply the `invlogit` transformation to the linear combination of predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3.math import invlogit\n",
    "\n",
    "with ivh_glm:\n",
    "        \n",
    "    p = invlogit(μ + α*bwt_kg + β*pneumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the sampling distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Bernoulli\n",
    "\n",
    "with ivh_glm:\n",
    "            \n",
    "    bb_like = Bernoulli('bb_like', p=p, observed=ivh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with ivh_glm:\n",
    "    \n",
    "    trace_ivh = sample(1000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_posterior(trace_ivh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions among variables\n",
    "\n",
    "Interactions imply that the effect of one covariate $X_1$ on $Y$ depends on the value of another covariate $X_2$.\n",
    "\n",
    "$$M(Y|X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 +\\beta_3 X_1 X_2$$\n",
    "\n",
    "the effect of a unit increase in $X_1$:\n",
    "\n",
    "$$M(Y|X_1+1, X_2) - M(Y|X_1, X_2)$$\n",
    "\n",
    "$$\\begin{align}\n",
    "&= \\beta_0 + \\beta_1 (X_1 + 1) + \\beta_2 X_2 +\\beta_3 (X_1 + 1) X_2\n",
    "- [\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 +\\beta_3 X_1 X_2] \\\\\n",
    "&= \\beta_1 + \\beta_3 X_2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = medals[medals.oecd==1].plot(x='log_population', y='medals', kind='scatter', alpha=0.8)\n",
    "medals[medals.oecd==0].plot(x='log_population', y='medals', kind='scatter', color='red', alpha=0.5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction can be interpreted as:\n",
    "\n",
    "- $X_1$ interacts with $X_2$\n",
    "- $X_1$ modifies the effect of $X_2$\n",
    "- $X_2$ modifies the effect of $X_1$\n",
    "- $X_1$ and $X_2$ are non-additive or synergistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a model that predicts medal count based on population size and OECD status, as well as the interaction. \n",
    "\n",
    "We can use the `dmatrix` function from the `patsy` library to set up a **design matrix**. This is a matrix of independent observations (rows) and predictors (columns) that defines the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "\n",
    "y = medals.medals\n",
    "X = dmatrix('log_population * oecd', data=medals)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is multiplied by the vector of model coefficients to generate an estimate of the response corresponding to each row.\n",
    "\n",
    "Now, fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interaction_params = fmin(poisson_loglike, [0,1,1,0], args=(X, y), maxiter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interaction_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything odd about these estimates?\n",
    "\n",
    "The main effect of the OECD effect is negative, which seems counter-intuitive. This is because the variable is interpreted as the OECD effect when the **log-population is zero**. This is not particularly meaningful.\n",
    "\n",
    "Asw we did with birth weight in the previous example, we can improve the interpretability of this parameter by centering the log-population variable prior to entering it into the model. This will result in the OECD main effect being interpreted as the marginal effect of being an OECD country for an **average-sized** country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = medals.medals\n",
    "X = dmatrix('center(log_population) * oecd', data=medals)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmin(poisson_loglike, [0,1,1,0], args=(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "How do we choose among competing models for a given dataset? More parameters are not necessarily better, from the standpoint of model fit. For example, fitting a 6th order polynomial to the LSD example certainly results in an overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poly(params, data):\n",
    "        x = np.c_[[data**i for i in range(len(params))]]\n",
    "        return np.dot(params, x)\n",
    "\n",
    "x, y = lsd_and_math.T.values\n",
    "    \n",
    "sum_squares_poly = lambda θ, x, y: np.sum((y - calc_poly(θ, x)) ** 2)\n",
    "betas = fmin(sum_squares_poly, np.zeros(7), args=(x,y), maxiter=1e6)\n",
    "plt.plot(x, y, 'ro')\n",
    "xvals = np.linspace(0, max(x), 100)\n",
    "plt.plot(xvals, calc_poly(betas, xvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach is to use an information-theoretic criterion to select the most appropriate model. For example **Akaike's Information Criterion (AIC)** balances the fit of the model (in terms of the likelihood) with the number of parameters required to achieve that fit. We can easily calculate AIC as:\n",
    "\n",
    "$$AIC = n \\log(\\hat{\\sigma}^2) + 2p$$\n",
    "\n",
    "where $p$ is the number of parameters in the model and $\\hat{\\sigma}^2 = RSS/(n-p-1)$.\n",
    "\n",
    "Notice that as the number of parameters increase, the residual sum of squares goes down, but the second term (a penalty) increases.\n",
    "\n",
    "AIC is a metric of **information distance** between a given model and a notional \"true\" model. Since we don't know the true model, the AIC value itself is not meaningful in an absolute sense, but is useful as a relative measure of model quality.\n",
    "\n",
    "To apply AIC to model selection, we choose the model that has the **lowest** AIC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "\n",
    "aic = lambda rss, p, n: n * np.log(rss/(n-p-1)) + 2*p\n",
    "\n",
    "RSS1 = sum_of_squares(fmin(sum_of_squares, [0,1], args=(x,y)), x, y)\n",
    "RSS2 = sum_squares_quad(fmin(sum_squares_quad, [1,1,-1], args=(x,y)), x, y)\n",
    "\n",
    "print('\\nModel 1: {0}\\nModel 2: {1}'.format(aic(RSS1, 2, n), aic(RSS2, 3, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, on the basis of \"information distance\", we would select the 2-parameter (linear) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Olympic medals model selection\n",
    "\n",
    "Use AIC to select the best model from the following set of Olympic medal prediction models:\n",
    "\n",
    "- population only\n",
    "- population and OECD\n",
    "- interaction model\n",
    "\n",
    "For these models, use the alternative form of AIC, which uses the log-likelhood rather than the residual sums-of-squares:\n",
    "\n",
    "$$AIC = -2 \\log(L) + 2p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Harrell, F. E. (2015). Regression Modeling Strategies (pp. 1–397).\n",
    "- Gelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
